{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896aeb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import re\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Conv3D, Conv3DTranspose, BatchNormalization, ReLU\n",
    "from tensorflow.python.keras.layers.convolutional import Conv3DTranspose\n",
    "from tensorflow.python.ops.init_ops_v2 import he_normal\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# from tensorflow.keras import mixed_precision\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903baefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clamp_disp(disp, min_disp, max_disp):\n",
    "    \"\"\"Clip max disparity, ortherwise it'll be hard for network to learn really big disparity/close object\"\"\"\n",
    "    return np.clip(disp, min_disp, max_disp)\n",
    "\n",
    "def _mean_std(img):\n",
    "    img = np.array(img, dtype=np.float32) / 255.0\n",
    "    img[:, :, 0] -= 0.485\n",
    "    img[:, :, 0] /= 0.229\n",
    "    img[:, :, 1] -= 0.456\n",
    "    img[:, :, 1] /= 0.224\n",
    "    img[:, :, 2] -= 0.406\n",
    "    img[:, :, 2] /= 0.225\n",
    "    \n",
    "    return img\n",
    "\n",
    "def StereoDataloader(img_left, img_right, disp_left, img_h, img_w, df_h, df_w, batch_num, ComPerBatch, data_ord, max_disp):\n",
    "    tmp_img = []\n",
    "    tmp_disp = []\n",
    "    if df_h > 32 :\n",
    "        randomH = np.random.randint(0, df_h)\n",
    "    else :\n",
    "        randomH = 0\n",
    "    if df_w > 32 :\n",
    "        randomW = np.random.randint(0, df_w)\n",
    "    else :\n",
    "        randomW = 0\n",
    "    \n",
    "    \n",
    "    for idx in range(batch_num*ComPerBatch,(batch_num+1)*ComPerBatch):\n",
    "        l = np.array(Image.open(img_left[data_ord[idx]]))[randomH:randomH+img_h, randomW:randomW+img_w, :]\n",
    "        r = np.array(Image.open(img_right[data_ord[idx]]))[randomH:randomH+img_h, randomW:randomW+img_w, :]\n",
    "        tmp_img.append(np.concatenate((_mean_std(l), _mean_std(r)), axis=2))\n",
    "        \n",
    "        dispL = np.loadtxt(disp_left[data_ord[idx]], delimiter=\",\", dtype=np.float32)[randomH:randomH+img_h, randomW:randomW+img_w]\n",
    "        tmp_disp.append(_clamp_disp(dispL,0,max_disp))\n",
    "    \n",
    "    return np.array(tmp_img, dtype=np.float32), np.array(tmp_disp, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b014927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(out_planes, kernel_size=kernel_size, strides=stride, padding='valid' if pad == 0 else 'same', dilation_rate=dilation, use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    ])\n",
    "\n",
    "def convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv3D(out_planes, kernel_size=kernel_size, strides=stride, padding='valid' if pad == 0 else 'same', dilation_rate=dilation, use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    ])\n",
    "\n",
    "class BasicBlock(tf.keras.Model):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.Sequential([\n",
    "            convbn(inplanes, planes, 3, stride, pad, dilation),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        out += x\n",
    "        return out\n",
    "\n",
    "class SoftArgMin(Layer):\n",
    "    def __init__(self, maxdisp):\n",
    "        super(SoftArgMin, self).__init__()        \n",
    "        self.disp_indices = tf.range(maxdisp, dtype=tf.float32)\n",
    "        self.disp_indices = tf.reshape(self.disp_indices, [1, maxdisp, 1, 1])\n",
    "\n",
    "    def call(self, x):\n",
    "        # [N, D, H, W] \n",
    "        x = tf.nn.softmax(x, axis=1)  # compute softmax over all disparity\n",
    "        x = tf.math.multiply(x, self.disp_indices)\n",
    "        # [N, D, H, W] -> [N, H, W]\n",
    "        x = tf.math.reduce_sum(x, axis=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class feature_extraction(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(feature_extraction, self).__init__()\n",
    "        self.inplanes = 32\n",
    "        self.firstconv = tf.keras.Sequential([\n",
    "            convbn(3, 32, 3, 2, 1, 1),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            convbn(32, 32, 3, 1, 1, 1),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            convbn(32, 32, 3, 1, 1, 1),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "        self.layer1 = self._make_layer(BasicBlock, 32, 3, 1, 1, 1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 64, 16, 2, 1, 1)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 128, 3, 1, 1, 1)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 128, 3, 1, 1, 2)\n",
    "        self.branch1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.AveragePooling2D(pool_size=(64, 64), strides=(64, 64)),\n",
    "            convbn(128, 32, 1, 1, 0, 1),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "        self.branch2 = tf.keras.Sequential([\n",
    "            tf.keras.layers.AveragePooling2D(pool_size=(32, 32), strides=(32, 32)),\n",
    "            convbn(128, 32, 1, 1, 0, 1),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "        self.branch3 = tf.keras.Sequential([\n",
    "            tf.keras.layers.AveragePooling2D(pool_size=(16, 16), strides=(16, 16)),\n",
    "            convbn(128, 32, 1, 1, 0, 1),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "        self.branch4 = tf.keras.Sequential([\n",
    "            tf.keras.layers.AveragePooling2D(pool_size=(8, 8), strides=(8, 8)),\n",
    "            convbn(128, 32, 1, 1, 0, 1),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "        self.lastconv = tf.keras.Sequential([\n",
    "            convbn(320, 128, 3, 1, 1, 1),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=1, padding='valid', strides=1, use_bias=False)\n",
    "        ])\n",
    "    \n",
    "    def _make_layer(self, block, planes, blocks, stride, pad, dilation):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = tf.keras.Sequential([\n",
    "                tf.keras.layers.Conv2D(planes * block.expansion, kernel_size=1, strides=stride, use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ])\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, pad, dilation))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, 1, None, pad, dilation))\n",
    "        \n",
    "        return tf.keras.Sequential(layers)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        output = self.firstconv(x)\n",
    "        output = self.layer1(output)\n",
    "        output_raw = self.layer2(output)\n",
    "        output = self.layer3(output_raw)\n",
    "        output_skip = self.layer4(output)\n",
    "        \n",
    "        output_branch1 = self.branch1(output_skip)\n",
    "        output_branch1 = tf.image.resize(output_branch1, (output_skip.shape[1], output_skip.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        output_branch2 = self.branch2(output_skip)\n",
    "        output_branch2 = tf.image.resize(output_branch2, (output_skip.shape[1], output_skip.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        output_branch3 = self.branch3(output_skip)\n",
    "        output_branch3 = tf.image.resize(output_branch3, (output_skip.shape[1], output_skip.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        output_branch4 = self.branch4(output_skip)\n",
    "        output_branch4 = tf.image.resize(output_branch4, (output_skip.shape[1], output_skip.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        output_feature = tf.concat((output_raw, output_skip, output_branch4, output_branch3, output_branch2, output_branch1), axis=3)\n",
    "        output_feature = self.lastconv(output_feature)\n",
    "        return output_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a67cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hourglass(Model):\n",
    "    def __init__(self, inplanes):\n",
    "        super(hourglass, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.Sequential([\n",
    "            Conv3D(inplanes*2, kernel_size=3, strides=2, padding='same'),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ])\n",
    "\n",
    "        self.conv2 = Conv3D(inplanes*2, kernel_size=3, strides=1, padding='same')\n",
    "\n",
    "        self.conv3 = tf.keras.Sequential([\n",
    "            Conv3D(inplanes*2, kernel_size=3, strides=2, padding='same'),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ])\n",
    "\n",
    "        self.conv4 = tf.keras.Sequential([\n",
    "            Conv3D(inplanes*2, kernel_size=3, strides=1, padding='same'),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ])\n",
    "\n",
    "        self.conv5 = tf.keras.Sequential([\n",
    "            Conv3DTranspose(inplanes*2, kernel_size=3, strides=2, padding='same'),\n",
    "            BatchNormalization()\n",
    "        ])\n",
    "\n",
    "        self.conv6 = tf.keras.Sequential([\n",
    "            Conv3DTranspose(inplanes, kernel_size=3, strides=2, padding='same'),\n",
    "            BatchNormalization()\n",
    "        ])\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x, presqu=None, postsqu=None):\n",
    "        out = self.conv1(x)\n",
    "        pre = self.conv2(out)\n",
    "        if postsqu is not None:\n",
    "            pre = tf.nn.relu(pre + postsqu)\n",
    "        else:\n",
    "            pre = tf.nn.relu(pre)\n",
    "\n",
    "        out = self.conv3(pre)\n",
    "        out = self.conv4(out)\n",
    "\n",
    "        if presqu is not None:\n",
    "            post = tf.nn.relu(self.conv5(out) + presqu)\n",
    "        else:\n",
    "            post = tf.nn.relu(self.conv5(out) + pre)\n",
    "\n",
    "        out = self.conv6(post)\n",
    "\n",
    "        return out, pre, post\n",
    "\n",
    "class PSMNet(Model):\n",
    "    def __init__(self, maxdisp):\n",
    "        super(PSMNet, self).__init__()\n",
    "        self.maxdisp = maxdisp\n",
    "        self.feature_extraction = feature_extraction()\n",
    "\n",
    "        self.dres0 = tf.keras.Sequential([\n",
    "            Conv3D(32, kernel_size=3, padding='same'), ReLU(),\n",
    "            Conv3D(32, kernel_size=3, padding='same'), ReLU()\n",
    "        ])\n",
    "\n",
    "        self.dres1 = tf.keras.Sequential([\n",
    "            Conv3D(32, kernel_size=3, padding='same'), ReLU(),\n",
    "            Conv3D(32, kernel_size=3, padding='same')\n",
    "        ])\n",
    "\n",
    "        self.dres2 = hourglass(32)\n",
    "        self.dres3 = hourglass(32)\n",
    "        self.dres4 = hourglass(32)\n",
    "\n",
    "        self.classif1 = tf.keras.Sequential([\n",
    "            Conv3D(32, kernel_size=3, padding='same'), ReLU(),\n",
    "            Conv3D(1, kernel_size=3, padding='same')\n",
    "        ])\n",
    "\n",
    "        self.classif2 = tf.keras.Sequential([\n",
    "            Conv3D(32, kernel_size=3, padding='same'), ReLU(),\n",
    "            Conv3D(1, kernel_size=3, padding='same')\n",
    "        ])\n",
    "\n",
    "        self.classif3 = tf.keras.Sequential([\n",
    "            Conv3D(32, kernel_size=3, padding='same'), ReLU(),\n",
    "            Conv3D(1, kernel_size=3, padding='same')\n",
    "        ])\n",
    "        \n",
    "        self.soft_argmin = SoftArgMin(self.maxdisp)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, data, training):\n",
    "        left = data[:,:,:,0:3]\n",
    "        right = data[:,:,:,3:6]        \n",
    "        refimg_fea = self.feature_extraction(left)\n",
    "        targetimg_fea = self.feature_extraction(right)\n",
    "        refimg_fea = tf.expand_dims(refimg_fea, axis=1)\n",
    "        targetimg_fea = tf.expand_dims(targetimg_fea, axis=1)\n",
    "        result = []\n",
    "        # Cost Volume\n",
    "        cost = []        \n",
    "        for d in range(max_disp // 4):\n",
    "            if d > 0:\n",
    "                left_shift = tf.pad(refimg_fea[:, :, :, d:, :], paddings=[[0, 0], [0, 0], [0, 0], [d, 0], [0, 0]])#, mode='SYMMETRIC')\n",
    "                right_shift = tf.pad(targetimg_fea[:, :, :, :-d, :], paddings=[[0, 0], [0, 0], [0, 0], [d, 0], [0, 0]])#, mode='SYMMETRIC')\n",
    "                cost_plate = tf.concat([left_shift, right_shift], axis=4)\n",
    "            else:                \n",
    "                cost_plate = tf.concat([refimg_fea, targetimg_fea], axis=4)\n",
    "            \n",
    "            cost.append(cost_plate)\n",
    "        \n",
    "        cost = tf.concat(cost, axis=1)\n",
    "        cost = tf.transpose(cost, perm=[0, 3, 2, 1, 4])        \n",
    "        \n",
    "        del right\n",
    "        del refimg_fea\n",
    "        del targetimg_fea\n",
    "        \n",
    "        # PSMNet Regularization\n",
    "        cost0 = self.dres0(cost)\n",
    "        cost0 = self.dres1(cost0) + cost0\n",
    "        \n",
    "        out1, pre1, post1 = self.dres2(cost0)\n",
    "        out1 = out1 + cost0\n",
    "\n",
    "        out2, pre2, post2 = self.dres3(out1, pre1, post1)\n",
    "        out2 = out2 + cost0\n",
    "\n",
    "        out3, pre3, post3 = self.dres4(out2, pre1, post2)\n",
    "        out3 = out3 + cost0\n",
    "\n",
    "        cost1 = self.classif1(out1)\n",
    "        cost2 = self.classif2(out2) + cost1\n",
    "        cost3 = self.classif3(out3) + cost2\n",
    "        cost1 = tf.squeeze(cost1, axis=-1)\n",
    "        cost2 = tf.squeeze(cost2, axis=-1)\n",
    "        cost3 = tf.squeeze(cost3, axis=-1)\n",
    "        \n",
    "        cost3 = tf.image.resize(cost3, (left.shape[2], left.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        cost3 = tf.image.resize(tf.transpose(cost3, perm=[0, 3, 2, 1]), (self.maxdisp, left.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        pred3 = self.soft_argmin(cost3)        \n",
    "        \n",
    "        if not net.trainable :\n",
    "            return pred3\n",
    "        else :\n",
    "            cost1 = tf.image.resize(cost1, (left.shape[2], left.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            cost1 = tf.image.resize(tf.transpose(cost1, perm=[0, 3, 2, 1]), (self.maxdisp, left.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            pred1 = self.soft_argmin(cost1)\n",
    "            cost2 = tf.image.resize(cost2, (left.shape[2], left.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            cost2 = tf.image.resize(tf.transpose(cost2, perm=[0, 3, 2, 1]), (self.maxdisp, left.shape[1]), method=tf.image.ResizeMethod.BILINEAR)            \n",
    "            pred2 = self.soft_argmin(cost2)\n",
    "            result = tf.concat([tf.expand_dims(pred1, axis=3), tf.expand_dims(pred2, axis=3), tf.expand_dims(pred3, axis=3)], axis=3)\n",
    "            return result\n",
    "        \n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred, beta=1.0, size_average=True, reduction='mean'):\n",
    "    abs_diff = tf.abs(y_true - y_pred)\n",
    "    squar_loss = tf.square(tf.minimum(abs_diff, beta))\n",
    "    linear_loss = tf.maximum(abs_diff - beta, 0.0)\n",
    "    loss = squar_loss + linear_loss\n",
    "\n",
    "    if size_average:\n",
    "        if reduction == 'mean':\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            loss = tf.reduce_sum(loss)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reduction type: {reduction}\")\n",
    "    return loss\n",
    "\n",
    "def MyLoss(y_true, y_pred):    \n",
    "    output1 = y_pred[:,:,:,0]\n",
    "    output2 = y_pred[:,:,:,1]\n",
    "    output3 = y_pred[:,:,:,2]\n",
    "    disp_true = y_true\n",
    "    mask = disp_true < 160\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    \n",
    "    weight1 = 0.5\n",
    "    weight2 = 0.7\n",
    "    weight3 = 1.0\n",
    "\n",
    "    loss = weight1 * smooth_l1_loss(disp_true*mask, output1*mask, size_average=True) + \\\n",
    "           weight2 * smooth_l1_loss(disp_true*mask, output2*mask, size_average=True) + \\\n",
    "           weight3 * smooth_l1_loss(disp_true*mask, output3*mask, size_average=True)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13eb7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading dataset path\n",
    "finl = open('gh_imgl.pkl','rb')\n",
    "finr = open('gh_imgr.pkl', 'rb')\n",
    "img_left = pickle.load(finl)\n",
    "img_right = pickle.load(finr)\n",
    "finl.close()\n",
    "finr.close()\n",
    "\n",
    "finl = open('gh_disp.pkl', 'rb')\n",
    "disp_left = pickle.load(finl)\n",
    "finl.close()\n",
    "\n",
    "#training/test data set\n",
    "tot_num1 = len(img_left)//2 # Half number, training data\n",
    "tot_num2 = len(img_left) # total number, training+test data\n",
    "file_max = tot_num2 # if training, file_max = tot_num1. Elif testing performance,file_max = tot_num2. \n",
    "\n",
    "# data size\n",
    "max_w = 984\n",
    "max_h = 560\n",
    "max_disp = 160\n",
    "\n",
    "img_w = 960\n",
    "img_h = 544\n",
    "df_w = max_w - img_w\n",
    "df_h = max_h - img_h\n",
    "\n",    
    "ComPerBatch = 1 #24 #256 # component number per batch\n",
    "batch_size = np.int32(tot_num2/ComPerBatch) # number of batch\n",
    "totpx_batch = img_w*img_h*ComPerBatch\n",
    "\n",
    "# if you want to save data, save_data = 1,\n",
    "#  else save_data = 0\n",
    "save_data = 1\n",
    "\n",
    "# Image save function\n",
    "def disp_img(img, title): \n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='jet')\n",
    "    plt.colorbar(shrink=0.5) \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "    gc.collect()\n",
    "\n",
    "vmin = 40\n",
    "vmax = 80\n",
    "vgap = 10\n",
    "# Image save function\n",
    "def _save_img(img, title, dest, file_name, data_ind):\n",
    "#     plt.switch_backend('Agg')\n",
    "    plt.rc('font', size=30)        # 기본 폰트 크기\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(img, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "    cbar.set_ticks(np.arange(vmin, vmax, vgap))  # 32 간격으로 눈금 설정    \n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig('%s/%s_%05d.png' %(dest,file_name,data_ind))\n",
    "    \n",
    "    plt.close()    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b5f99e-e340-4663-b3f9-0d712a7068e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2470/2470"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48388"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PSMNet performance #############################################################################\n",
    "max_epochs1 = 196\n",
    "chkdir='saved_model/PSMNet_iter%d/' %(max_epochs1)\n",
    "\n",
    "net = PSMNet(maxdisp=max_disp)\n",
    "net.compile(optimizer=tf.keras.optimizers.Adam(), loss=MyLoss)\n",
    "net.trainable = True\n",
    "\n",
    "batch_n = 0\n",
    "mix_set = np.arange(file_max) #if training : mix_set = np.random.permutation(file_max)\n",
    "\n",
    "DB_imgs,DB_disp = StereoDataloader(\n",
    "    img_left = img_left,\n",
    "    img_right = img_right,\n",
    "    disp_left = disp_left,                \n",
    "    img_h = img_h,\n",
    "    img_w = img_w,\n",
    "    df_h = df_h,\n",
    "    df_w = df_w,\n",
    "    batch_num = batch_n, # order of batch\n",
    "    ComPerBatch = ComPerBatch, # component number of a batch\n",
    "    data_ord = mix_set,\n",
    "    max_disp = max_disp\n",
    ")\n",
    "    \n",
    "net.fit(DB_imgs, DB_disp, batch_size=1, epochs=1, verbose=0)\n",
    "net.load_weights(chkdir)\n",
    "net.trainable = False #True\n",
    "\n",
    "dest_dir='PSMNet_iter%d' %(max_epochs1)\n",
    "\n",
    "if not os.path.isdir(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "\n",
    "file_name = '%s/test.csv' %(dest_dir)\n",
    "f = open(file_name, 'w')    \n",
    "f.write('Data No./Number as Disparity Difference, PSMNet_GH2470_epc%d loss, ratio of px<=1, <=2, <=3\\n' %(max_epochs1))\n",
    "f.close()\n",
    "\n",
    "for batch_n in range(batch_size):\n",
    "    DB_imgs,DB_disp = StereoDataloader(\n",
    "        img_left = img_left,\n",
    "        img_right = img_right,\n",
    "        disp_left = disp_left,                \n",
    "        img_h = img_h,\n",
    "        img_w = img_w,\n",
    "        df_h = df_h,\n",
    "        df_w = df_w,\n",
    "        batch_num = batch_n, # order of batch\n",
    "        ComPerBatch = ComPerBatch, # component number of a batch\n",
    "        data_ord = mix_set,\n",
    "        max_disp = max_disp\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    if save_data == 0:\n",
    "        print('Batch %d/%d' %(batch_n+1,batch_size))\n",
    "        result = net.predict(DB_imgs, batch_size=1, verbose=1)\n",
    "        result = _clamp_disp(result,0, max_disp)\n",
    "    elif save_data == 1:\n",
    "        print('\\rBatch %d/%d' %(batch_n+1,batch_size), end='')\n",
    "        result = _clamp_disp(net.predict(DB_imgs, batch_size=1, verbose=0),0, max_disp)\n",
    "        loss1 = np.mean(np.abs(DB_disp-result))\n",
    "\n",
    "        file_name = '%s/test.csv' %(dest_dir)\n",
    "        f = open(file_name, 'a')\n",
    "\n",
    "        disp_diff1 = np.abs(DB_disp-result)\n",
    "\n",
    "        f.write('%d, ' %(batch_n+1))\n",
    "\n",
    "        # Model disparity error and px area ratio (<= 1px,2px,3px)        \n",
    "        f.write('%.5f, ' %loss1)        \n",
    "        f.write('%.5f, ' %(np.sum(disp_diff1<=1)/totpx_batch))        \n",
    "        f.write('%.5f, ' %(np.sum(disp_diff1<=2)/totpx_batch))        \n",
    "        f.write('%.5f \\n'%(np.sum(disp_diff1<=3)/totpx_batch))\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        #### Disparity calculated by Trueth Data            \n",
    "        title = 'Ground Truth_Data#%d' %(batch_n+1)\n",
    "        file_name = 'True_Disp'\n",
    "        _save_img(DB_disp[0,:,:], title, dest_dir, file_name, batch_n+1)\n",
    "\n",
    "        #### Disparity calculated by Stereo Camera Data with Model\n",
    "        title = 'PSMNet_Data#%d' %(batch_n+1)\n",
    "        file_name = 'Stereo_Disp'\n",
    "        _save_img(result[0,:,:], title, dest_dir, file_name, batch_n+1)\n",
    "\n",
    "        del file_name\n",
    "        del loss1\n",
    "        del disp_diff1\n",
    "        gc.collect()\n",
    "\n",
    "    del DB_imgs\n",
    "    del DB_disp\n",
    "    del result\n",
    "    gc.collect()\n",
    "    \n",
    "del net\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
