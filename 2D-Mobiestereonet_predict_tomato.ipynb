{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896aeb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import re\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Conv3D, Conv3DTranspose, BatchNormalization, ReLU\n",
    "from tensorflow.python.keras.layers.convolutional import Conv3DTranspose\n",
    "from tensorflow.python.ops.init_ops_v2 import he_normal\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# from tensorflow.keras import mixed_precision\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903baefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clamp_disp(disp, min_disp, max_disp):\n",
    "    \"\"\"Clip max disparity, ortherwise it'll be hard for network to learn really big disparity/close object\"\"\"\n",
    "    return np.clip(disp, min_disp, max_disp)\n",
    "\n",
    "def _mean_std(img):\n",
    "    img = np.array(img, dtype=np.float32) / 255.0\n",
    "    img[:, :, 0] -= 0.485\n",
    "    img[:, :, 0] /= 0.229\n",
    "    img[:, :, 1] -= 0.456\n",
    "    img[:, :, 1] /= 0.224\n",
    "    img[:, :, 2] -= 0.406\n",
    "    img[:, :, 2] /= 0.225\n",
    "    \n",
    "    return img\n",
    "\n",
    "def StereoDataloader(img_left, img_right, disp_left, img_h, img_w, df_h, df_w, batch_num, ComPerBatch, data_ord, max_disp):\n",
    "    tmp_img = []\n",
    "    tmp_disp = []\n",
    "    if df_h > 32 :\n",
    "        randomH = np.random.randint(0, df_h)\n",
    "    else :\n",
    "        randomH = 0\n",
    "    if df_w > 32 :\n",
    "        randomW = np.random.randint(0, df_w)\n",
    "    else :\n",
    "        randomW = 0\n",
    "    \n",
    "    for idx in range(batch_num*ComPerBatch,(batch_num+1)*ComPerBatch):\n",
    "        l = np.array(Image.open(img_left[data_ord[idx]]))[randomH:randomH+img_h, randomW:randomW+img_w,0:3]\n",
    "        r = np.array(Image.open(img_right[data_ord[idx]]))[randomH:randomH+img_h, randomW:randomW+img_w,0:3]\n",
    "        dispL = np.loadtxt(disp_left[data_ord[idx]], delimiter=\",\", dtype=np.float32)[randomH:randomH+img_h, randomW:randomW+img_w]\n",
    "        \n",
    "        tmp_img.append(np.concatenate((_mean_std(l), _mean_std(r)), axis=2))\n",
    "        tmp_disp.append(_clamp_disp(dispL,0,max_disp))\n",
    "        \n",
    "    return np.array(tmp_img, dtype=np.float32), np.array(tmp_disp, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614baaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "\"\"\" Fundamental Building Blocks \"\"\"\n",
    "###############################################################################\n",
    "\n",
    "def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(out_planes, kernel_size=kernel_size, strides=stride, padding='valid' if pad == 0 else 'same', dilation_rate=dilation, use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    ])\n",
    "\n",
    "def convbn_dws(inp, oup, kernel_size, stride, pad, dilation, second_relu=True):\n",
    "    padding = dilation if dilation > 1 else pad\n",
    "    if second_relu:\n",
    "        return tf.keras.Sequential([\n",
    "            # dw\n",
    "            tf.keras.layers.DepthwiseConv2D(kernel_size=kernel_size, strides=stride, padding='SAME' if padding > 0 else 'VALID',\n",
    "                                           dilation_rate=dilation, use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(max_value=6.0),  # ReLU6\n",
    "            # pw\n",
    "            tf.keras.layers.Conv2D(oup, kernel_size=1, strides=1, padding='VALID', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(max_value=6.0) # ReLU6\n",
    "        ])\n",
    "    else:\n",
    "        return tf.keras.Sequential([\n",
    "            # dw\n",
    "            tf.keras.layers.DepthwiseConv2D(kernel_size=kernel_size, strides=stride, padding='SAME' if padding > 0 else 'VALID',\n",
    "                                           dilation_rate=dilation, use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(max_value=6.0), # ReLU6\n",
    "            # pw\n",
    "            tf.keras.layers.Conv2D(oup, kernel_size=1, strides=1, padding='VALID', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ])\n",
    "    \n",
    "class MobileV1_Residual(tf.keras.layers.Layer):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride, downsample, pad, dilation, **kwargs):\n",
    "        super(MobileV1_Residual, self).__init__(**kwargs)\n",
    "        self.stride = stride\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = convbn_dws(inplanes, planes, 3, stride, pad, dilation)\n",
    "        self.conv2 = convbn_dws(planes, planes, 3, 1, pad, dilation, second_relu=False)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        out = self.add([out, x])\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileV2_Residual(tf.keras.layers.Layer):\n",
    "    def __init__(self, inp, oup, stride, expanse_ratio, dilation=1, **kwargs):\n",
    "        super(MobileV2_Residual, self).__init__(**kwargs)\n",
    "        self.stride = stride\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "        self.pad = dilation\n",
    "\n",
    "        hidden_dim = int(inp * expanse_ratio)\n",
    "\n",
    "        if expanse_ratio == 1:\n",
    "            self.conv = tf.keras.Sequential([\n",
    "                # dw\n",
    "                tf.keras.layers.DepthwiseConv2D(kernel_size=3, strides=stride, padding='SAME', dilation_rate=dilation, use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(max_value=6.0), # ReLU6\n",
    "                # pw-linear\n",
    "                tf.keras.layers.Conv2D(oup, kernel_size=1, strides=1, padding='VALID', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "            ])\n",
    "        else:\n",
    "            self.conv = tf.keras.Sequential([\n",
    "                # pw\n",
    "                tf.keras.layers.Conv2D(hidden_dim, kernel_size=1, strides=1, padding='VALID', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(max_value=6.0), # ReLU6\n",
    "                # dw\n",
    "                tf.keras.layers.DepthwiseConv2D(kernel_size=3, strides=stride, padding='SAME', dilation_rate=dilation, use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(max_value=6.0), # ReLU6\n",
    "                # pw-linear\n",
    "                tf.keras.layers.Conv2D(oup, kernel_size=1, strides=1, padding='VALID', use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "            ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return self.add([x, self.conv(x)])\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class feature_extraction(tf.keras.Model):\n",
    "    def __init__(self, add_relus=False):\n",
    "        super(feature_extraction, self).__init__()\n",
    "\n",
    "        self.expanse_ratio = 3\n",
    "        self.inplanes = 32\n",
    "\n",
    "        if add_relus:\n",
    "            self.firstconv = tf.keras.Sequential([\n",
    "                MobileV2_Residual(3, 32, 2, self.expanse_ratio),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                MobileV2_Residual(32, 32, 1, self.expanse_ratio),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                MobileV2_Residual(32, 32, 1, self.expanse_ratio),\n",
    "                tf.keras.layers.ReLU()\n",
    "            ])\n",
    "        else:\n",
    "            self.firstconv = tf.keras.Sequential([\n",
    "                MobileV2_Residual(3, 32, 2, self.expanse_ratio),\n",
    "                MobileV2_Residual(32, 32, 1, self.expanse_ratio),\n",
    "                MobileV2_Residual(32, 32, 1, self.expanse_ratio)\n",
    "            ])\n",
    "\n",
    "        self.layer1 = self._make_layer(MobileV1_Residual, 32, 3, 1, 1, 1)\n",
    "        self.layer2 = self._make_layer(MobileV1_Residual, 64, 16, 2, 1, 1)\n",
    "        self.layer3 = self._make_layer(MobileV1_Residual, 128, 3, 1, 1, 1)\n",
    "        self.layer4 = self._make_layer(MobileV1_Residual, 128, 3, 1, 1, 2)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride, pad, dilation):\n",
    "        downsample = None\n",
    "\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = tf.keras.Sequential([\n",
    "                tf.keras.layers.Conv2D(planes, kernel_size=1, strides=stride, use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ])\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample, pad, dilation)]\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, 1, None, pad, dilation))\n",
    "\n",
    "        return tf.keras.Sequential(layers)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.firstconv(x)\n",
    "        x = self.layer1(x)\n",
    "        l2 = self.layer2(x)\n",
    "        l3 = self.layer3(l2)\n",
    "        l4 = self.layer4(l3)\n",
    "\n",
    "        feature_volume = tf.concat([l2, l3, l4], axis=-1)  # Changed from dim=1 to axis=-1\n",
    "\n",
    "        return feature_volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea931f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "\"\"\" Cost Volume Related Functions \"\"\"\n",
    "###############################################################################\n",
    "\n",
    "def interweave_tensors(refimg_fea, targetimg_fea):\n",
    "    # TensorFlow doesn't have a direct equivalent to refimg_fea.new_zeros.\n",
    "    # We create a zero tensor with the desired shape.\n",
    "    B, H, W, C = refimg_fea.shape # Changed order to channels_last for tensorflow\n",
    "    \n",
    "    interwoven_features = []\n",
    "    for i in range(2*W):\n",
    "        if i % 2 == 0:  # even index\n",
    "            interwoven_features.append(refimg_fea[:, :, i//2, :])\n",
    "        else:  # odd index\n",
    "            interwoven_features.append(targetimg_fea[:, :, i//2, :])\n",
    "            \n",
    "    # change List to Tensor\n",
    "    interwoven_features = tf.stack(interwoven_features, axis=2)\n",
    "    \n",
    "    return interwoven_features\n",
    "\n",
    "###############################################################################\n",
    "\"\"\" Disparity Regression Function \"\"\"\n",
    "###############################################################################\n",
    "\n",
    "def disparity_regression(x, maxdisp):\n",
    "    disp_values = tf.range(0, maxdisp, dtype=x.dtype)\n",
    "    disp_values = tf.reshape(disp_values, [1, maxdisp, 1, 1]) # Reshape for broadcasting\n",
    "    \n",
    "    return tf.reduce_sum(tf.math.multiply(x, disp_values), axis=1) # Changed axis for tensorflow\n",
    "\n",
    "###############################################################################\n",
    "\"\"\" Loss Function \"\"\"\n",
    "###############################################################################\n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred, beta=1.0, size_average=True, reduction='mean'):\n",
    "    abs_diff = tf.abs(y_true - y_pred)\n",
    "    squar_loss = tf.square(tf.minimum(abs_diff, beta))\n",
    "    linear_loss = tf.maximum(abs_diff - beta, 0.0)\n",
    "    loss = squar_loss + linear_loss\n",
    "\n",
    "    if size_average:\n",
    "        if reduction == 'mean':\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            loss = tf.reduce_sum(loss)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reduction type: {reduction}\")\n",
    "    return loss\n",
    "\n",
    "def MyLoss(y_true, y_pred):\n",
    "    output0 = y_pred[:,:,:,0]\n",
    "    output1 = y_pred[:,:,:,1]\n",
    "    output2 = y_pred[:,:,:,2]\n",
    "    output3 = y_pred[:,:,:,3]\n",
    "    disp_true = y_true\n",
    "    mask = disp_true < 160\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    \n",
    "    weight0 = 0.5\n",
    "    weight1 = 0.5\n",
    "    weight2 = 0.7\n",
    "    weight3 = 1.0\n",
    "\n",
    "    loss = weight0 * smooth_l1_loss(disp_true*mask, output0*mask, size_average=True) + \\\n",
    "           weight1 * smooth_l1_loss(disp_true*mask, output1*mask, size_average=True) + \\\n",
    "           weight2 * smooth_l1_loss(disp_true*mask, output2*mask, size_average=True) + \\\n",
    "           weight3 * smooth_l1_loss(disp_true*mask, output3*mask, size_average=True)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e60147",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "\"\"\" Disparity Regression Function : 2D\"\"\"\n",
    "###############################################################################\n",
    "\n",
    "class hourglass2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channels, **kwargs):\n",
    "        super(hourglass2D, self).__init__(**kwargs)\n",
    "\n",
    "        self.expanse_ratio = 2\n",
    "        self.conv1 = MobileV2_Residual(in_channels, in_channels * 2, stride=2, expanse_ratio=self.expanse_ratio)\n",
    "        self.conv2 = MobileV2_Residual(in_channels * 2, in_channels * 2, stride=1, expanse_ratio=self.expanse_ratio)\n",
    "        self.conv3 = MobileV2_Residual(in_channels * 2, in_channels * 4, stride=2, expanse_ratio=self.expanse_ratio)\n",
    "        self.conv4 = MobileV2_Residual(in_channels * 4, in_channels * 4, stride=1, expanse_ratio=self.expanse_ratio)\n",
    "        self.conv5 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2DTranspose(in_channels * 2, kernel_size=3, padding='same', strides=2, use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ])\n",
    "\n",
    "        self.conv6 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2DTranspose(in_channels, kernel_size=3, padding='same', strides=2, use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ])\n",
    "\n",
    "        self.redir1 = MobileV2_Residual(in_channels, in_channels, stride=1, expanse_ratio=self.expanse_ratio)\n",
    "        self.redir2 = MobileV2_Residual(in_channels * 2, in_channels * 2, stride=1, expanse_ratio=self.expanse_ratio)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "\n",
    "        conv5 = tf.nn.relu(self.conv5(conv4) + self.redir2(conv2))\n",
    "        conv6 = tf.nn.relu(self.conv6(conv5) + self.redir1(x))\n",
    "\n",
    "        return conv6\n",
    "\n",
    "class MSNet2D(tf.keras.Model):\n",
    "    def __init__(self, maxdisp):\n",
    "        super(MSNet2D, self).__init__()\n",
    "\n",
    "        self.maxdisp = maxdisp\n",
    "        self.num_groups = 1\n",
    "        self.volume_size = 48\n",
    "        self.hg_size = 48\n",
    "        self.dres_expanse_ratio = 3\n",
    "        self.feature_extraction = feature_extraction(add_relus=True)\n",
    "\n",
    "        self.preconv11 = tf.keras.Sequential([\n",
    "            convbn(320, 256, 1, 1, 0, 1),\n",
    "            layers.ReLU(),\n",
    "            convbn(256, 128, 1, 1, 0, 1),\n",
    "            layers.ReLU(),\n",
    "            convbn(128, 64, 1, 1, 0, 1),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv2D(32, (1, 1), strides=(1, 1), padding='same')\n",
    "        ])\n",
    "\n",
    "        self.conv3d = tf.keras.Sequential([\n",
    "            layers.Conv3D(16, (8, 3, 3), strides=(8, 1, 1), padding='same'), # Note: 'same' padding handles different sizes\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv3D(32, (4, 3, 3), strides=(4, 1, 1), padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv3D(16, (2, 3, 3), strides=(2, 1, 1), padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "\n",
    "        self.volume11 = tf.keras.Sequential([\n",
    "            convbn(16, 1, 1, 1, 0, 1),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "\n",
    "        self.dres0 = tf.keras.Sequential([\n",
    "            MobileV2_Residual(self.volume_size, self.hg_size, 1, self.dres_expanse_ratio),\n",
    "            layers.ReLU(),\n",
    "            MobileV2_Residual(self.hg_size, self.hg_size, 1, self.dres_expanse_ratio),\n",
    "            layers.ReLU()\n",
    "        ])\n",
    "\n",
    "        self.dres1 = tf.keras.Sequential([\n",
    "            MobileV2_Residual(self.hg_size, self.hg_size, 1, self.dres_expanse_ratio),\n",
    "            layers.ReLU(),\n",
    "            MobileV2_Residual(self.hg_size, self.hg_size, 1, self.dres_expanse_ratio)\n",
    "        ])\n",
    "\n",
    "        self.encoder_decoder1 = hourglass2D(self.hg_size)\n",
    "        self.encoder_decoder2 = hourglass2D(self.hg_size)\n",
    "        self.encoder_decoder3 = hourglass2D(self.hg_size)\n",
    "\n",
    "        self.classif0 = tf.keras.Sequential([\n",
    "            convbn(self.hg_size, self.hg_size, 3, 1, 1, 1),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv2D(self.hg_size, (3, 3), padding='same', strides=(1,1)) # No bias in original, but TF conv2d has bias by default.\n",
    "        ])\n",
    "        self.classif1 = tf.keras.Sequential([\n",
    "            convbn(self.hg_size, self.hg_size, 3, 1, 1, 1),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv2D(self.hg_size, (3, 3), padding='same', strides=(1,1))\n",
    "        ])\n",
    "        self.classif2 = tf.keras.Sequential([\n",
    "            convbn(self.hg_size, self.hg_size, 3, 1, 1, 1),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv2D(self.hg_size, (3, 3), padding='same', strides=(1,1))\n",
    "        ])\n",
    "        self.classif3 = tf.keras.Sequential([\n",
    "            convbn(self.hg_size, self.hg_size, 3, 1, 1, 1),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv2D(self.hg_size, (3, 3), padding='same', strides=(1,1))\n",
    "        ])\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, data, training):  # Use call method for forward pass\n",
    "        L = data[:,:,:,0:3]\n",
    "        R = data[:,:,:,3:6]\n",
    "        \n",
    "        features_L = self.feature_extraction(L)\n",
    "        features_R = self.feature_extraction(R)\n",
    "        \n",
    "        featL = self.preconv11(features_L)\n",
    "        featR = self.preconv11(features_R)\n",
    "        \n",
    "        B, C, H, W = featL.shape  # Get shape\n",
    "        \n",
    "        # Cost Volume\n",
    "        volume = []  \n",
    "        \n",
    "        for d in range(self.volume_size):\n",
    "            if d > 0:\n",
    "                cost_plate = interweave_tensors(tf.pad(featL[:, :, d:, :], paddings=[[0, 0], [0, 0], [d, 0], [0, 0]]), tf.pad(featR[:, :, :-d, :], paddings=[[0, 0], [0, 0], [d, 0], [0, 0]]))\n",
    "                cost_plate = tf.expand_dims(cost_plate, axis=1) # TF uses expand_dims\n",
    "                cost_plate = self.conv3d(cost_plate)\n",
    "                cost_plate = tf.squeeze(cost_plate, axis=1)\n",
    "                cost_plate = self.volume11(cost_plate)\n",
    "            else:                \n",
    "                cost_plate = interweave_tensors(featL, featR)\n",
    "                cost_plate = tf.expand_dims(cost_plate, axis=1) # TF uses expand_dims\n",
    "                cost_plate = self.conv3d(cost_plate)\n",
    "                cost_plate = tf.squeeze(cost_plate, axis=1)\n",
    "                cost_plate = self.volume11(cost_plate)\n",
    "            volume.append(cost_plate)\n",
    "        \n",
    "        volume = tf.concat(volume, axis=0)\n",
    "        volume = tf.transpose(volume, perm=[3, 1, 2, 0])\n",
    "\n",
    "        del R\n",
    "        del features_L\n",
    "        del features_R\n",
    "        del featL\n",
    "        del featR\n",
    "        \n",
    "        cost0 = self.dres0(volume)\n",
    "        cost0 = self.dres1(cost0) + cost0\n",
    "        \n",
    "        del volume\n",
    "        \n",
    "        out1 = self.encoder_decoder1(cost0)\n",
    "        out2 = self.encoder_decoder2(out1)\n",
    "        out3 = self.encoder_decoder3(out2)\n",
    "        \n",
    "        if net.trainable :\n",
    "            cost0 = self.classif0(cost0)\n",
    "            cost1 = self.classif1(out1)\n",
    "            cost2 = self.classif2(out2)\n",
    "            cost3 = self.classif3(out3)\n",
    "\n",
    "            cost0 = tf.image.resize(cost0, (L.shape[1], L.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            cost0 = tf.image.resize(tf.transpose(cost0, perm=[0, 3, 1, 2]), (self.maxdisp, L.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            pred0 = tf.nn.softmax(cost0, axis=1)\n",
    "            pred0 = disparity_regression(pred0, self.maxdisp)\n",
    "\n",
    "            cost1 = tf.image.resize(cost1, (L.shape[1], L.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            cost1 = tf.image.resize(tf.transpose(cost1, perm=[0, 3, 1, 2]), (self.maxdisp, L.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            pred1 = tf.nn.softmax(cost1, axis=1)\n",
    "            pred1 = disparity_regression(pred1, self.maxdisp)\n",
    "\n",
    "            cost2 = tf.image.resize(cost2, (L.shape[1], L.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            cost2 = tf.image.resize(tf.transpose(cost2, perm=[0, 3, 1, 2]), (self.maxdisp, L.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            pred2 = tf.nn.softmax(cost2, axis=1)\n",
    "            pred2 = disparity_regression(pred2, self.maxdisp)\n",
    "            \n",
    "            cost3 = tf.image.resize(cost3, (L.shape[1], L.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            cost3 = tf.image.resize(tf.transpose(cost3, perm=[0, 3, 1, 2]), (self.maxdisp, L.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            pred3 = tf.nn.softmax(cost3, axis=1)\n",
    "            pred3 = disparity_regression(pred3, self.maxdisp)\n",
    "            \n",
    "            result = tf.concat([tf.expand_dims(pred0, axis=3), tf.expand_dims(pred1, axis=3), tf.expand_dims(pred2, axis=3), tf.expand_dims(pred3, axis=3)], axis=3)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        else :\n",
    "            cost3 = self.classif3(out3)\n",
    "            cost3 = tf.image.resize(cost3, (L.shape[1], L.shape[2]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            cost3 = tf.image.resize(tf.transpose(cost3, perm=[0, 3, 1, 2]), (self.maxdisp, L.shape[1]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "            pred3 = tf.nn.softmax(cost3, axis=1)\n",
    "            pred3 = disparity_regression(pred3, self.maxdisp)\n",
    "\n",
    "            return [pred3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13eb7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading dataset path\n",
    "finl = open('gh_imgl.pkl','rb')\n",
    "finr = open('gh_imgr.pkl', 'rb')\n",
    "img_left = pickle.load(finl)\n",
    "img_right = pickle.load(finr)\n",
    "finl.close()\n",
    "finr.close()\n",
    "\n",
    "finl = open('gh_disp.pkl', 'rb')\n",
    "disp_left = pickle.load(finl)\n",
    "finl.close()\n",
    "\n",
    "#training/test data set\n",
    "tot_num1 = len(img_left)//2 # Half number, training data\n",
    "tot_num2 = len(img_left) # total number, training+test data\n",
    "file_max = tot_num2 # if training, file_max = tot_num1. Elif testing performance,file_max = tot_num2. \n",
    "\n",
    "# data size\n",
    "max_w = 984\n",
    "max_h = 560\n",
    "max_disp = 160\n",
    "\n",
    "img_w = 960\n",
    "img_h = 544\n",
    "df_w = max_w - img_w\n",
    "df_h = max_h - img_h\n",
    "\n",
    "feature_n = 32 # Default Feature Number of GC-Net\n",
    "ComPerBatch = 1 #24 #256 # component number per batch\n",
    "batch_size = np.int32(tot_num2/ComPerBatch) # number of batch\n",
    "totpx_batch = img_w*img_h*ComPerBatch\n",
    "\n",
    "# Image save function\n",
    "def disp_img(img, title): \n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='jet')\n",
    "    plt.colorbar(shrink=0.5) \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "    gc.collect()\n",
    "\n",
    "# Depth limitation of graph\n",
    "vmin = 40\n",
    "vmax = 80\n",
    "vgap = 10\n",
    "\n",
    "# Image save function\n",
    "def _save_img(img, title, dest, file_name, data_ind):\n",
    "#     plt.switch_backend('Agg')\n",
    "    plt.rc('font', size=30)\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(img, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "    cbar.set_ticks(np.arange(vmin, vmax, vgap))\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig('%s/%s_%05d.png' %(dest,file_name,data_ind))\n",
    "    \n",
    "    plt.close()    \n",
    "    gc.collect()"
   ]
  }
   ],
   "source": [
    "# 2D-Mobiestereonet Performance\n",
    "img_w_hf = img_w//2\n",
    "img_h_hf = img_h//2\n",
    "\n",
    "ComPerBatch = 1 #24 #256 # component number per batch\n",
    "batch_size = np.int32(tot_num2/ComPerBatch) # number of batch\n",
    "totpx_batch = img_w*img_h*ComPerBatch\n",
    "\n",
    "net = MSNet2D(maxdisp=max_disp)\n",
    "net.compile(optimizer=tf.keras.optimizers.Adam(), loss=MyLoss)\n",
    "\n",
    "epc_n = 99\n",
    "chkdir=\"ckpt_endpoint_GH_epc%d/\" %(epc_n+1)\n",
    "net.load_weights(chkdir)\n",
    "mix_set = np.arange(tot_num2)\n",
    "\n",
    "DB_imgs,DB_disp = StereoDataloader(\n",
    "    img_left = img_left,\n",
    "    img_right = img_right,\n",
    "    disp_left = disp_left,                \n",
    "    img_h = img_h_hf,\n",
    "    img_w = img_w_hf,\n",
    "    df_h = df_h,\n",
    "    df_w = df_w,\n",
    "    batch_num = 0, # order of batch\n",
    "    ComPerBatch = ComPerBatch, # component number of a batch\n",
    "    data_ord = mix_set,\n",
    "    max_disp = max_disp\n",
    ")\n",
    "\n",
    "net.trainable = True\n",
    "net.fit(DB_imgs, DB_disp, batch_size=1, epochs=1, verbose=0)\n",
    "\n",
    "dest_dir=\"GH_epc%d/\" %(epc_n+1)\n",
    "\n",
    "if not os.path.isdir(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "    \n",
    "file_name = '%s/test.csv' %(dest_dir)\n",
    "f = open(file_name, 'w')    \n",
    "f.write('Data No./Number as Disparity Difference, 2D-MSNet_GH2470_epc%d loss, ratio of px<=1, <=2, <=3\\n' %(max_epochs1))\n",
    "f.close()\n",
    "\n",
    "print(\"Process State-------------------------------------------------------\")\n",
    "for batch_n in range(batch_size):\n",
    "    DB_imgs,DB_disp = StereoDataloader(\n",
    "        img_left = img_left,\n",
    "        img_right = img_right,\n",
    "        disp_left = disp_left,                \n",
    "        img_h = img_h,\n",
    "        img_w = img_w,\n",
    "        df_h = df_h,\n",
    "        df_w = df_w,\n",
    "        batch_num = batch_n, # order of batch\n",
    "        ComPerBatch = ComPerBatch, # component number of a batch\n",
    "        data_ord = mix_set,\n",
    "        max_disp = max_disp\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    print('\\rBatch %d/%d' %(batch_n+1,batch_size), end='')\n",
    "    for area_i in range(4):\n",
    "        if area_i == 0 :\n",
    "            DB_disp_t = DB_disp[:,:img_h_hf,:img_w_hf]\n",
    "            DB_imgs_t = DB_imgs[:,:img_h_hf,:img_w_hf,:]\n",
    "\n",
    "        elif area_i == 1 :\n",
    "            DB_disp_t = DB_disp[:,:img_h_hf,img_w_hf:img_w]\n",
    "            DB_imgs_t = DB_imgs[:,:img_h_hf,img_w_hf:img_w,:]\n",
    "\n",
    "        elif area_i == 2 :\n",
    "            DB_disp_t = DB_disp[:,img_h_hf:img_h,:img_w_hf]\n",
    "            DB_imgs_t = DB_imgs[:,img_h_hf:img_h,:img_w_hf,:]\n",
    "\n",
    "        else :\n",
    "            DB_disp_t = DB_disp[:,img_h_hf:img_h,img_w_hf:img_w]\n",
    "            DB_imgs_t = DB_imgs[:,img_h_hf:img_h,img_w_hf:img_w,:]\n",
    "\n",
    "        result = _clamp_disp(net.predict(DB_imgs_t, batch_size=1, verbose=1),0, max_disp)[0,:,:,:]\n",
    "\n",
    "        if area_i == 0 or area_i == 2 :\n",
    "            result_tmp = result\n",
    "\n",
    "        if area_i == 1 :\n",
    "            top_row = tf.concat([result_tmp, result], axis=2)\n",
    "\n",
    "        if area_i == 3 :\n",
    "            bottom_row = tf.concat([result_tmp, result], axis=2)\n",
    "\n",
    "        loss1 = loss1 + np.mean(np.abs(DB_disp_t-result))\n",
    "\n",
    "    result = tf.concat([top_row, bottom_row], axis=1)\n",
    "\n",
    "    loss1 = loss1/4\n",
    "\n",
    "    print('Disparity calculate process : loss = %.5f' %(loss1))\n",
    "    \n",
    "    file_name = '%s/test.csv' %(dest_dir)\n",
    "    f = open(file_name, 'a')\n",
    "\n",
    "    disp_diff1 = np.abs(DB_disp-result)\n",
    "\n",
    "    f.write('%d, ' %(batch_n+1))\n",
    "\n",
    "    # Model disparity error and px area ratio (<= 1px,2px,3px)        \n",
    "    f.write('%.5f, ' %loss1)        \n",
    "    f.write('%.5f, ' %(np.sum(disp_diff1<=1)/totpx_batch))        \n",
    "    f.write('%.5f, ' %(np.sum(disp_diff1<=2)/totpx_batch))        \n",
    "    f.write('%.5f \\n'%(np.sum(disp_diff1<=3)/totpx_batch))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    #### Disparity calculated by Trueth Data            \n",
    "    title = 'Ground Truth_Data#%d' %(batch_n+1)\n",
    "    file_name = 'True_Disp'\n",
    "    _save_img(DB_disp[0,:,:], title, dest_dir, file_name, batch_n+1)\n",
    "\n",
    "    #### Disparity calculated by Stereo Camera Data with Model\n",
    "    title = '2D-MSNet_Data#%d' %(batch_n+1)\n",
    "    file_name = 'Stereo_Disp'\n",
    "    _save_img(result[0,:,:], title, dest_dir, file_name, batch_n+1)\n",
    "\n",
    "    del file_name\n",
    "    del loss1\n",
    "    del disp_diff1\n",
    "    gc.collect()\n",
    "\n",
    "    del DB_imgs\n",
    "    del DB_disp\n",
    "    del result\n",
    "    gc.collect()\n",
    "    \n",
    "# # Print and Save Model summary\n",
    "# net.summary()\n",
    "# with open('2D-MSNet_GH2470_summary.txt', 'w') as f:\n",
    "#     net.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    \n",
    "del net\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3ac78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
